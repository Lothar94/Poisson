%%
% Modificación de una plantilla de Latex para adaptarla al castellano.
%%

%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 10pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage[usenames,dvipsnames]{color} % Coloring code
\usepackage{wrapfig} % Allows in-line images
\usepackage[utf8]{inputenc}
\usepackage{enumerate}
\usepackage{enumitem}

% Imágenes
\usepackage{graphicx} 

\usepackage{amsmath}
% para importar svg
%\usepackage[generate=all]{svgfig}

% sudo apt-get install texlive-lang-spanish
\usepackage[spanish]{babel} % English language/hyphenation
\selectlanguage{spanish}
% Hay que pelearse con babel-spanish para el alineamiento del punto decimal
\decimalpoint
\usepackage{dcolumn}
\newcolumntype{d}[1]{D{.}{\esperiod}{#1}}
\makeatletter
\addto\shorthandsspanish{\let\esperiod\es@period@code}
\makeatother

\usepackage{longtable}
\usepackage{tabu}
\usepackage{supertabular}

\usepackage{multicol}
\newsavebox\ltmcbox

% Para algoritmos
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}

% Para matrices
\usepackage{amsmath}

% Símbolos matemáticos
\usepackage{amssymb}
\usepackage{accents}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\usepackage[hidelinks]{hyperref}

\usepackage[section]{placeins} % Para gráficas en su sección.
\usepackage[T1]{fontenc} % Required for accented characters
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\setlength{\parindent}{0pt}
\parskip=8pt
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography
\newcommand{\imagen}[2]{\begin{center} \includegraphics[width=90mm]{#1} \\#2 \end{center}}
\newcommand{\RFC}[1]{\href{https://www.ietf.org/rfc/rfc#1.txt}{RFC-#1}}

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{center} % Center align
{\Huge\@title} % Increase the font size of the title
\end{center}

\vspace{20pt} % Some vertical space between the title and author name

\begin{flushright} % Right align
{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
\renewcommand{\baselinestretch}{0.5}

}
%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Distribución de Poisson}\\ % Title
\vspace{20 pt}
} % Subtitle

\author{\textsc{Rafael Nogales Vaquero\\
Lothar Soto Palma} % Author
\\{\textit{Universidad de Granada}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------
%\setcounter{secnumdepth}{3}
\usepackage{anysize}
\marginsize{3cm}{3cm}{2.5cm}{2.5cm}

\newcounter{def}
\newcounter{teo}


\begin{document}
\maketitle
\tableofcontents
\setcounter{page}{1}
\pagebreak
\section{Deducción de función de densidad}
La distribución de Poisson expresa a partir de una frecuencia de ocurrencia media, la probabilidad de que ocurra un determinado número de eventos durante cierto periodo de tiempo. Se especializa en la probabilidad de ocurrencia de sucesos con probabilidades pequeñas. Es una distribución que sirve para modelar diferentes tipos de experimentos, como fenómenos en los que se espera que ocurra un suceso en específico como esperar el bus, o la llegada de clientes en deternimado servicio.

La distribución de Poisson es un caso particular del limite de la distribución binomial, podemos deducirlo de la siguiente representación geométrica:\\
\begin{center}
\includegraphics[scale=0.6]{1.png} 
\end{center}
Tomamos $AB$ como el segmento de longitud $L$ y $CD$ el segmento de longitud $l$ contenido en $AB$, la probabilidad de que un punto se encuentre en $CD$ es $l/L$. Si tomamos n puntos aleatorios en $AB$ la probabilidad de que exactamente $x$ de ellos esten en $CD$ viene dada por una distribución binomial:
$$B(x|\frac{l}{L},n)=\frac{n!}{x!(n-x)!}(\frac{l}{L})^x(1-\frac{l}{L})^{n-x}$$
Ahora hacemos que n y L crezcan indefinidamente de tal manera que el número promedio de puntos por unidad de longitud es un número finito $k \neq 0$, $\frac{n}{L}\rightarrow k$.
$$B(x|\frac{l}{L},n)=\frac{n(n-1)...(n-x+1)}{x!n^x}(\frac{nl}{L})^x(1-\frac{n}{L}\frac{l}{n})^{n-x}$$
asi que el limite de $B(x|\frac{l}{L},n)$ cuando $n,L\rightarrow \infty$:
$$\lim_{n,L\rightarrow \infty}B(x|\frac{l}{L},n) = \lim_{n,L\rightarrow \infty}\frac{1(1-\frac{1}{n})...(1-\frac{x+1}{n})}{x!}(\frac{nl}{L})^x(1-\frac{n}{L}\frac{l}{n})^{n-x}=\frac{(kl)^xe^{-kl}}{x!}$$
Tomando que $kl=\theta$ obtenemos la expresión de la distribución de Poisson:
$$Poisson(\theta)=f(x|\theta)=\dfrac{e^{-\theta}\theta^x}{x!},\ \ \ x=0,1...$$
Tiene un único parámetro $\theta$ se suele denominar parámetro de intensidad, y se aplica sobre una variable aleatoria $X$.

Es interesante conocer previamente el desarrollo en serie de Taylor para $e^x$, ya que este desarrollo se usará por ejemplo en el cálculo de la función generatriz de momentos o en ver la buena definición de la función de densidad, tenemos que es $e^x=\sum_{i=0}^{\infty}\frac{x^i}{i!}$.

La función de densidad está bién definida consideramos la suma de todos los experimentos x:
$$\sum_{x=0}^{\infty}\frac{e^{-\theta}\theta^x}{x!}=e^{-\theta}\sum_{x=0}^{\infty}\frac{\theta^x}{x!}=e^{-\theta}e^{\theta}=1$$
\section{Función generatriz de momentos}


La función generatriz de momentos se define como sigue:
$$\phi(t)=E[e^{-tX}]=\sum_{x=0}^{\infty}e^{-t x}\frac{e^{-\theta}\theta^x}{x!}=e^{-\theta}\sum_{x=0}^{\infty}e^{t x}\frac{\theta^x}{x!}=e^{-\theta}\sum_{x=0}^{\infty}\frac{(\theta e^t)^x}{x!}=e^{-\theta}e^{\theta e^t}=e^{\theta(e^t-1)}$$
\subsection{Esperanza}
Para el calculo de la esperanza tan solo debemos derivar la función generatriz de momentos una vez y evaluarla con t=0:
$$\frac{\partial\phi}{\partial t} = \theta e^t e^{\theta(e^t-1)}$$
Evaluamos la expresión en t=0 y tenemos que:
$$E[X]=\theta$$
\subsection{Varianza}
La varianza se puede expresar como $\sigma^2=E[X^2]-E[X]^2$ por lo que es tan solo calcular el momento de orden 2 usando la función generatriz de momentos, por lo que volvemos a derivar la expresión anterior:
$$\frac{\partial^2\phi}{\partial t^2} = \theta e^t e^{\theta(e^t-1)}+\theta^2 e^{2t} e^{\theta(e^t-1)}$$
Evaluamos la expresión en t=0 y tenemos que:
$$E[X^2]=\theta+\theta^2$$
Y por lo tanto la expresión de la varianza es la siguiente:
$$\sigma^2=\theta+\theta^2-\theta^2=\theta$$
\section{Estimador máximo verosimil}
Para una muestra de tamaño $n$, que denotamos por $x=(x_1,...,x_n)$ definimos la función de verosimilitud como:
$$l(x_1,...,x_n|\theta)=\prod_{i=1}^{n} \frac{e^{-\theta}\theta^{x_i}}{x_i!}=e^{-n\theta}\prod_{i=1}^{n} \frac{\theta^{x_i}}{x_i!}$$
Para el cálculo del estimador máximo verosimil de la ley de Poisson vamos a maximizar la función $log(l(x_1,...,x_n|\theta))$ puesto que el logaritmo es creciente conserva los máximos:
$$L(x_1,...,x_n|\theta)=log(l(x_1,...,x_n|\theta))=-n\theta + \sum_{i=1}^{n}log(\frac{\theta^{x_i}}{x_i!})=-n\theta+ \sum_{i=1}^{n}(x_i log(\theta)-log(x_i!))$$
Ahora calculamos la derivada para maximizar el logaritmo de la función de verosimilitud:
$$\frac{\partial L}{\partial \theta}=-n+\sum_{i=1}^{n}\frac{x_i}{\theta}=-n+\frac{1}{\theta}\sum_{i=1}^{n}x_i$$
Encontramos el extremo igualando a 0:
$$-n+\frac{1}{\theta}\sum_{i=1}^{n}x_i=0 \Leftrightarrow \frac{1}{\theta}\sum_{i=1}^{n}x_i=n \Leftrightarrow \theta =\frac{\sum_{i=1}^{n}x_i}{n}$$
Para verificar que hemos encontrado un máximo calculamos la segunda derivada y evaluamos en $\theta =\frac{\sum_{i=1}^{n}x_i}{n}$:
$$\frac{\partial^2 L}{\partial \theta^2}=-\frac{1}{\theta^2}\sum_{i=1}^{n}x_i$$
$$\frac{\partial^2 L}{\partial \theta^2}(\frac{\sum_{i=1}^{n}x_i}{n})<0$$
Se tiene que la segunda derivada es negativa ya que la suma de variables $x_i$ es siempre positiva y $\theta$ está elevado al cuadrado por lo que siempre es positiva, por lo que no hay ningún valor que haga positiva la segunda derivada y por tanto $\hat{\theta}=\frac{\sum_{i=1}^{n}x_i}{n}$ es un estimador máximo verosimil para la ley de Poisson.
\subsection{Insesgadez}
\addtocounter{def}{1}
\emph{Definición \arabic{def}}: Se denomina sesgo de un estimador a la diferencia entre la esperanza del estimador y el verdadero valor del parámetro a estimar. Un estimador es insesgado si su sesgo es nulo por ser su esperanza igual al parámetro que se desea estimar.

Sea una variable aleatoria que sigue una distribución de Poisson de parametro $\theta$ y consideramos n pruebas $x=(x_1,...x_n)$, para que el estimador $\hat{\theta}$ sea insesgado tenemos que probar que:
$$E[\hat{\theta}]=\theta$$
\textbf{Demostración}:
$$E[\hat{\theta}]=E[\frac{1}{n}\sum_{i=1}^{n}x_i]=\frac{1}{n}\sum_{i=1}^{n}E[x_i]=\frac{1}{n}\sum_{i=1}^{n}\theta=\frac{n\theta}{n}=\theta$$
\subsection{Eficiencia}
\addtocounter{def}{1}
\emph{Definición \arabic{def}}: Un estimador $\hat{\theta}_1$ se dice que es más eficiente que otro estimador $\hat{\theta}_2$, si la varianza del primero es menor que la del segundo $Var(\hat{\theta}_1)<Var(\hat{\theta}_2)$.

Sea una variable aleatoria que sigue una distribución de Poisson de parametro $\theta$ y consideramos n pruebas $x=(x_1,...x_n)$, para que el estimador $\hat{\theta}$ sea eficiente tenemos que probar que:
$$Var(\hat{\theta})=\frac{1}{nI(\theta)}$$
Se verifica que el estimador tiene el mismo valor que la cota de Cramer-Rao.\\
\textbf{Demostración}:
Calculamos la función información de Fisher para ello en primer lugar veamos cual es el valor del logaritmo de la función de densidad: 
$$log(f(x|\theta))=-\theta + xlog(\theta) -log(x!)$$
Derivamos y elevamos al cuadrado previamente para el calculo de la información de Fisher:
$$(\frac{\partial}{\partial\theta}log(f(x|\theta)))^2=(\frac{x-\theta}{\theta})^2$$
Calculamos la esperanza de lo anterior:
$$I(\theta)=E[(\frac{\partial}{\partial\theta}log(f(x|\theta)))^2]=E[(\frac{x-\theta}{\theta})^2]=\frac{1}{\theta^2}E[(x-\theta)^2]=\frac{1}{\theta^2}Var(x)=\frac{\theta}{\theta^2}=\frac{1}{\theta}$$
Puesto que tenemos $n$ experimentos:
$$nI(\theta)=\frac{n}{\theta}$$
Calculamos la varianza del estimador para ver si coincide con la información de Fisher:
$$Var(\hat{\theta})=Var(\frac{\sum_{i=1}^{n}x_i}{n})=\frac{1}{n^2}Var(\sum_{i=1}^{n}x_i)=\frac{n\theta}{n^2}=\frac{\theta}{n}$$
Por lo que se verifica la condición de de eficiencia y el estimador es eficiente.
\subsection{Consistencia}
\addtocounter{teo}{1}
\emph{Teorema \arabic{teo}}: Una condición suficiente para que $\hat{\theta}$ sea un estimador consistente es que dicho estimador tiene que verificar:
\begin{description}
\item $E[\hat{\theta}] \rightarrow \theta$
\item $Var(\hat{\theta}) \rightarrow 0$
\end{description}
Con $n \rightarrow \infty$.

Sea una variable aleatoria que sigue una distribución de Poisson de parametro $\theta$ y consideramos n pruebas $x=(x_1,...x_n)$, el estimador $\hat{\theta}$ es consistente ya que sabemos que dicho estimador es insesgado, y conocemos al valor de la varianza del mismo:
$$E[\hat{\theta}]=\theta$$
$$Var(\hat{\theta})=\frac{\theta}{n} \rightarrow_{P_\theta} 0, \ \ \ n\rightarrow \infty$$
\subsection{Suficiencia}
\addtocounter{teo}{1}
\emph{Teorema \arabic{teo}}: Sean $X_1, X_2, ..., X_n$ variables aleatorias independientes con una función distribución de distribución conjunta $f(x_1, x_2, ..., x_n | \theta)$ que depende del parámetro $\theta$. \\
Entonces se dice que el estadístico $Y = u(x_1, ..., x_n)$ es suficiente para $\theta$ si y solamente si $f(x_1, x_2, ..., x_n | \theta)$ se puede factorizar de la siguiente forma: \\
$$ f(x_1,...,x_n| \theta) = \Phi(u(x_1, ..., x_n) | \theta)\cdot h(x_1, ..., x_n) $$
Veamos ahora que el estadístico \(T(X) = \sum_{i=0}^n x_i\) es un estadístico suficiente para el parámetro $\theta$ de la distribución de Poisson: \\
Consideremos en primer lugar la probabilidad conjunta de la distribución:
$$P(X=x) = P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) $$
Teniendo en cuenta que en nuestra distribución de Poisson se tiene que: \(P(X=k) = \frac{e^{-\theta }\theta ^{k}}{k!}\) y que las observaciones son independientes podemos reescribirlo como:
$$P(X=x) = \prod_{i=0}^nP(X_i = x_i) = \frac{e^{-\theta }\theta ^{x_1}}{x_1!}\cdot \frac{e^{-\theta }\theta ^{x_2}}{x_2!}\cdot ... \cdot \frac{e^{-\theta }\theta ^{x_n}}{x_n!}$$
Simplificando tenemos:
$$f(x_1...x_n | \theta) = \prod_{i=0}^nP(X_i = x_i) = e^{-n\theta }\theta ^{\sum_{i=0}^nx_i} \frac{1}{\prod_{i=1}^nx_i!}$$
Donde vemos que hemos conseguido factorizar $f(x_1...x_n | \theta)$ como en el teorema; en nuestro caso $u(X) = T(X) = \sum_{i_0}^n x_i$ y $h(X) =  \frac{1}{\prod_{i=1}^nx_i!}$\\
Para finalizar conviene resaltar que hemos probado que el estadístico $T(X)$ es suficiente para $\theta$, pero nosotros lo que queremos es ver que $\hat{\theta}$ es suficiente para $\theta$.\\
Pero esto es trivial ya que $\hat{\theta} = \frac{1}{n}\sum_{i=0}^n = \frac{1}{n}T(X)$ y podemos encontrar una nueva función $\hat u(x_1,...,x_n)$ tal que:
$$f(x_1...x_n | \theta) = \prod_{i=0}^nP(X_i = x_i) = e^{-n\theta }\theta ^{n}\theta ^{\frac{1}{n}{\sum_{i=0}^nx_i}} \frac{1}{\prod_{i=1}^nx_i!}$$ 
Que cumple el esquema: \\
$$ f(x_1,...,x_n| \theta) = \Phi(u(\hat\theta) | \theta)\cdot h(x_1, ..., x_n) $$

Donde: $\Phi(u(\hat\theta) | \theta) = \prod_{i=0}^nP(X_i = x_i) = e^{-n\theta }\theta ^{n}\theta ^{\frac{1}{n}{\sum_{i=0}^nx_i}}$ \\
Y $h(x_1, ..., x_n) = \frac{1}{\prod_{i=1}^nx_i!}$

\section{Ejemplos}
\textbf{Calcular el número de particulas $\alpha$ emitidas en una fuente radiactiva: \\
}Experimentalmente se obtiene que el numero medio de particulas $\alpha$ emitidas en dos minutos es 3.
¿Cual es la probabilidad de que en los próximos dos minutos el número de partículas $\alpha$ sea 0, 1, 2, 3 ó 4?\\
¿Y de que sea mayor o igual que cinco?\\
\textbf{Solución:}\\
La probabilidad de que $x$ partículas se emitan en un periodo de dos minutos se puede modelar mediante una función de distribución de Poisson:
$$ P_{\mu}(x) =\frac{\mu^x}{x!}e^{-\mu} $$
Donde $\mu$ es el valor promedio de particulas emitido en dos minutos.\\
En nuestro caso $\mu = 1.5\cdot2 = 3$ 
Calculemos ahora $P_{\mu}(i)$ para $i=0,1,2,3,4$\\
$$P_{\mu}(0) = \frac{3^0}{0!}e^{-3} = e^{-3} = 0.05 $$
$$P_{\mu}(1) = \frac{3^1}{1!}e^{-3} = 3e^{-3} = 0.15 $$
$$P_{\mu}(2) = \frac{3^2}{2!}e^{-3} = \frac{9}{2}e^{-3} = 0.22 $$
$$P_{\mu}(3) = \frac{3^3}{3!}e^{-3} = \frac{9}{2}e^{-3} = 0.22 $$
$$P_{\mu}(4) = \frac{3^4}{4!}e^{-3} = \frac{9}{2}e^{-3} = 0.17 $$
$$\sum_{i=5}^{+\infty}P_{\mu}(i) = \sum_{i=5}^{+\infty}\frac{3^i}{i!}e^{-3} = \sum_{i=0}^{+\infty}\frac{3^i}{i!}e^{-3} - \sum_{i=0}^{5}\frac{3^i}{i!}e^{-3} = e^3e^{-3} - \sum_{i=0}^{5}\frac{3^i}{i!}e^{-3} = 1 - 0.05 - 0.15 - 0.22 - 0.22 - 0.17 = 0.19$$



\section{Referencias}
\end{document}
